behaviors:
  Player:
    trainer_type: ppo
    max_steps: 50000000
    hyperparameters:
      batch_size: 128
      buffer_size: 1024
      learning_rate: 0.0003
      beta: 0.01
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
      vis_encode_type: simple
      memory:
        sequence_length: 64
        memory_size: 256
    reward_signals:
      gail:
        gamma: 0.99
        strength: 1
        network_settings:
          normalize: false
          hidden_units: 64
          num_layers: 1
          vis_encode_type: simple
        learning_rate: 0.0003
        use_actions: false
        use_vail: false
        demo_path: ../Assets/Demonstrations/RRreward45minutes.demo
    behavioral_cloning:
      demo_path: ../Assets/Demonstrations/RRreward45minutes.demo
      steps: 0
      strength: 0.8
      samples_per_update: 1024
torch_settings:
  device: cuda
engine_settings:
  no_graphics: true
  time_scale: 1
